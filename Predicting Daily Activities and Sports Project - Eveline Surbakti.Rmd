---
title: "ML PROJECT"
author: "Eveline Surbakti - 19200629"
date: "April 15, 2020"
output: word_document
---

```{r, include=FALSE}
setwd("D:/2020 - Spring/STAT40970  Machine Learning & AI (online)/Assignment/PROJECT")
load("D:/2020 - Spring/STAT40970  Machine Learning & AI (online)/Assignment/PROJECT/data_activity_recognition.RData")

#- An alternative approach to discarding sensor is aggregating in some way the measures of these sensors. For example, each body part has 3 sensor types recording 3 directions x-y-z. 
#You can aggregate over the 3 sensor types and compute some summary quantity giving the general movement over the directions x-y-z (for example computing the mean).

library(keras)  # for fitting 
library(tfruns) # for additional grid search & model training functions
library(tensorflow)

# prepare data
dim(x_train)
x_train<-data.matrix(array_reshape(x_train,c(nrow(x_train),125*45)))
x_test=data.matrix(array_reshape(x_test,c(nrow(x_test),125*45)))
dim(x_train)
dim(x_test)

dim(y_train)
dim(y_test)

y_train=data.matrix(data.frame(y_train))
y_test=data.matrix(data.frame(y_test))

dim(y_train)
dim(y_test)

y_train=to_categorical(y_train-1)
y_test=to_categorical(y_test-1)

dim(y_train)
dim(y_test)

# normalize input data to 0 - 1
range_norm <- function(x, a = 0, b = 1) {
  ( (x - min(x)) / (max(x) - min(x)) )*(b - a) + a }

x_train=apply(x_train,2,range_norm)
x_test=apply(x_test,2,range_norm)


dim(x_train)
dim(x_test)

N=nrow(x_train)
X=ncol(x_train)

# split the test data in two - validation and actual testing
val <- sample(1:nrow(x_test), 500)
test <- setdiff(1:nrow(x_test), val)
x_val <- x_test[val,]
y_val <- y_test[val,]
x_test <- x_test[test,]
y_test <- y_test[test,]

# for batch size
N <- nrow(x_train)
bs <- round(N)*0.02


```

```{r}
#checking the data
range(x_train)
dim(x_test)
plot(x_test[c(1:50)], type = 'l')

```

```{r}
#the dimensions of the training set
N =nrow(x_train)
V =ncol(x_train)
```
Hyperparameter tuning:
```{r}
# run ---------------------------------------------------------------
#hyperparameter tuning values
dropout_set_1 <- c(0.4, 0.3)
dropout_set_2 <- c(0.2, 0.1)
size_set_1 <- c(5625,2813,1406)
size_set_2 <- c(2813,1406,703)
optim_set <- c("sgd", "adam")

runs <- tuning_run("model2.R",
                   runs_dir = "result",
                   flags = list(
                     dropout_1 = dropout_set_1,
                     dropout_2 = dropout_set_2,
                     size_1 = size_set_1,
                     size_2 = size_set_2,
                     optimizer = optim_set), sample = 0.01) 
# run all combinations

```

```{r}
#before
library(tfruns)
# hyperparameter tuning values
# run ---------------------------------------------------------------
set1_set=c(5625,1406)
set2_set=c(5625,1406)
dropout_set=c(0,0.3)

# tuning procedure
runs <- tuning_run("model1.R",
                   runs_dir = "final",
                   flags = list(
                     set1 = set1_set,
                     set2 = set1_set, 
                   dropout = dropout_set))

```
The sgd optimizer is employed for learning and early stopping is employed with patience = 25. 

Stochastic gradient descent optimizer with support for momentum, learning rate decay, and Nesterov momentum.

In the training procedure, we can use early stopping to avoid overfitting. In Keras, early stopping can be specified bymeans of a callback, which is a set of functions to be applied at given stages of the training procedure. To do so, we use the argument callaback, which takes in input a list with callbacks. The function implementing early stopping during training is callback_early_stopping. Among the arguments of this function, patience sets the number of epochs with no improvement after which training will be stopped. We set it to 25, and we consider as a criterion to be monitored the value of the accuracy on the test data, by specifying monitor = "val_accuracy". Note that a value of patience too small could lead to underfitting.

```{r}
read_metrics <- function(path, files = NULL)
# 'path' is where the runs are --> e.g. "path/to/runs"
{path <- paste0(path, "/")
  if ( is.null(files) ) files <- list.files(path)
  n <- length(files)
  out <- vector("list", n)
    for ( i in 1:n ) 
      {
    dir <- paste0(path, files[i], "/tfruns.d/")
    out[[i]] <- jsonlite::fromJSON(paste0(dir, "metrics.json"))
    out[[i]]$flags <- jsonlite::fromJSON(paste0(dir, "flags.json"))
  }
return(out)
}

plot_learning_curve <- function(x, ylab = NULL, cols = NULL, top = 3,span=2)
{ smooth_line <- function(y)  # to add a smooth line to points
    {x <- 1:length(y)
     out <- predict(loess(y ~ x, span = span))
  return(out)
}

matplot(x, ylab = ylab, xlab = "Epochs", type = "n",...)
grid()
matplot(x, pch = 19, col = adjustcolor(cols, 0.3), add = TRUE)
tmp <- apply(x, 2, smooth_line)
tmp <- sapply( tmp, "length<-", max(lengths(tmp)) )
set <- order(apply(tmp, 2, max, na.rm = TRUE), decreasing = TRUE)[1:top]
cl <- rep(cols, ncol(tmp))
cl[set] <- "deepskyblue2"
matlines(tmp, lty = 1, col = cl, lwd = 2)}
```

Graph
Looking at the graph below,we can say that almost all the models have their validation accuracy between 90-95%. The curves in blue represent the models with the top 4 highest validation accuraies at convergence.
```{r}
library(reprex)
library(jsonlite)
# extract results
out <- read_metrics("runs_result")
# extract validation accuracy and plot learning curve
acc <- sapply(out, "[[", "val_accuracy")
plot_learning_curve(acc[[2]], col = adjustcolor("black", 0.3),ylab = "Val accuracy", top = 3)
```

Best model:
```{r}
res = ls_runs(metric_val_accuracy > 0.0,
              runs_dir = "resulttest4", order = metric_val_accuracy)

res = res[,c(2,4,8:11)]
res[1:15,] # top 10 models
tensorboard("resulttest3")


#write.csv(out, "D:/2020 - Spring/STAT40970  Machine Learning & AI (online)/Assignment/Assignment 3/outfinal.csv")

plot(fit)
```
